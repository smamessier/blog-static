

[
  
  
    
    
      {
        "title": "Soccer",
        "excerpt": "Football (Soccer for US readers) has always been a big part of my life and I can’t survive a week without touching the ball.\n",
        "content": "I’ve been playing soccer since childhood, first at a competitive level and then in the Supaero varsity team. Later on, I played for the Airbus corporate team in Munich and I’m now enjoying joining pick-ups at Georgia Tech on Friday nights. There are two main reasons why I just love soccer - and this can apply to many other team sports - firstly it teaches you how to be a good teammate, i.e work hard for the team, forget about your personal objectives and serve a higher goal. Arriving in a new team is always an interesting challenge, you first have to find your place in the existing schema, express and adapt your skills to complement the other players. The half-time discussion is also a very good exercise as you only have a couple of minutes to find a solution based on everyone’s remarks and your own analysis of the game. Secondly it involves and trains your physical, psychological and cognitive abilities in a unique way, at different levels, engaging together raw reflexes, motion planning and tactical thinking in conscious and unconscious ways. ",
        "url": "/2009/05/26/soccer/"
      },
    
      {
        "title": "Astronomy",
        "excerpt": "A summary of my practice of astronomy and astrophotography\n",
        "content": "I enjoy practicing astronomy and astro-photography. I see it as a way to experience the communion of science and nature, directly with my own eyes or through mirrors and lenses. Astro photography is a slightly more demanding hobby as it requires good tracking and image processing capabilities. A good hike if often required to find a good observation spot which is also part of the fun. I used Astronomy at higher scientific level twice, once for an exoplanet detection project about which I started writing a blog post and the next year as a research project at Supaero to detect asteroids crashing on the Moon.  ",
        "url": "/2009/05/26/astronomy/"
      },
    
      {
        "title": "Piloting",
        "excerpt": "Where I describe my short experience as a private pilot\n",
        "content": "I got my Private Pilot License in 2013 and was mostly flying in the South of France. I took my flight lessons at the Supaero-ISAE flight school in Lasbordes (near Toulouse).  ",
        "url": "/2009/05/26/piloting/"
      },
    
      {
        "title": "In-flight performance measurements of the TB-20",
        "excerpt": "This project was very exciting as we performed in-flight performance measurement while maneuvering the TB-20 around the Lasbordes airfield. The school TB-20 is equipped with additional aerodynamic and inertial sensors.\nThe report is in french, I don’t have time right now to translate it into english, it’s quite easy to understand for people with flight dynamics background as it handles classical measurements:\n\n\n  Calibration of altimeter and airspeed indicator (Altimètre, anémomètre)\n  Climb rate calculation (Taux de montée)\n  Leveled acceleleration (Accélération en palier)\n  Stalling, buffeting\n\n\nHere is the full report presentation:\n\n\n",
        "content": "This project was very exciting as we performed in-flight performance measurement while maneuvering the TB-20 around the Lasbordes airfield. The school TB-20 is equipped with additional aerodynamic and inertial sensors. The report is in french, I don’t have time right now to translate it into english, it’s quite easy to understand for people with flight dynamics background as it handles classical measurements:  Calibration of altimeter and airspeed indicator (Altimètre, anémomètre) Climb rate calculation (Taux de montée) Leveled acceleleration (Accélération en palier) Stalling, buffeting Here is the full report presentation: ",
        "url": "/2010/05/25/in-flight-performance-measurements-of-the-tb-20/"
      },
    
      {
        "title": "Bbox Hackathon - Magic Dashboard",
        "excerpt": "I participated to the Bbox Hackathon and got the 2nd prize. The idea was to create a 2nd-screen app for a set-top-box.\n\n\n",
        "content": "I participated to the Bbox Hackathon and got the 2nd prize. The idea was to create a 2nd-screen app for a set-top-box. ",
        "url": "/2010/05/26/bbox-hackathon-magic-dashboard/"
      },
    
      {
        "title": "Computer Vision – Personal thoughts",
        "excerpt": "Being part of the following class projects motivated me to write a couple of thoughts about Computer Vision and my understanding of it coming with an Engineering background.\n\nProjects\n\n\n  3D Body Scanner\n  Smartphone app for safe driving\n  Compact database of line drawings\n\n\nComputer science and engineering\n\nI always considered Computer Science as a hobby or a technological tool supporting engineering or theoretical research. Back in France, in the so-called Grande Ecoles (engineering schools), the computing department is only here to teach every student basic programming skills. I personally think that every curious and open-minded engineering student should have learned these programming skills by himself years ago. For example, I was surprised to state the small number of aerospace students with a sharp understanding of object-oriented programming. I had a few smartphone apps projects in mind and I wanted to meet people to brainstorm about these projects. That’s one of the reason I wanted to take classes in the College of Computing.\n\nI had my first contact with computer vision as an astronomy enthusiast when I had to get clean pictures from noisy low-resolution videos of Saturn. The pipeline was the following: choosing the best images out of one thousand, registering, then stacking them and use wavelets as a final touch. Astronomers are not that good in computer vision. Most of them use opaque processing software and sleep during the processing.\n Before I entered Georgia Tech, I had a very interesting research project about detecting light flashes caused by asteroids impacting the moon from very noisy and dark videos. So my first job was to register the video. I thought it would be as easy as it was with pictures of planets. But cross-correlation based techniques just didn’t work. That’s why I came to read advanced computer vision literature. I couldn’t imagine that so much had been done about image registration. That’s one of the reasons that pushed me to follow CS7495 this semester. Moreover, I thought it would be awesome to combine two of my favorite subjects: Mathematics and Computer science.\nI was still thinking I had a good computing background before I started to talk with my classmates. SIFT, SURF, RANSAC, SVM, HOG, all these things sounded like chinese to me. It was quite difficult for me to understand the subtleties of the first lectures, but I caught up quite fast and got fascinated by lots of different topics.\n\nComputers vs Humans\n\nBefore taking this class, it wasn’t clear to me why/when or how the human and the computer could have different approaches to solve a computer vision problem. In the cases where the computer approach was totally different from the human one, was it because of a lack of resources, learning capacity… or was it simply a more judicious choice? I used to consider detection methods that aren’t available to humans (because of computational load for example) as cheating. I always thought there should be some easier method…  But reading one of the chapter of Forsyth and Ponce – Computer Vision about Gestalt principles of perceptions made me realize that there were too many processes and resources in human’s brain to be able (nowadays) to identify them and implement them properly. Things like image segmentation look like basic tasks for us, we don’t even think about it.\n\nProbabilistic vs Deterministic\n\nMy first project took advantage of the Kinect 3d sensor to perform unsupervised 3d body scans. One of the most challenging part was non-rigid registration of 3d point clouds. One of the papers I read about it was Anguelov’s “The correlated correspondence algorithm for unsupervised registration of nonrigid surface”. It was the first time I read about bayesian probabilities applied to computer vision problems. I took me a couple of days to be able to understand it completely but it was worth a shot. It led me to understand MRFs and explore belief propagation algorithms and more generally structured model-based methods. Before that I was mostly used to deterministic methods.\n\n\n\nRecognition paradigm\n\nObject recognition is one of these computer vision problems where the human is still performing much better than the machine. State-of-the art research seems to tend to the following paradigm:\n\n\n  Gradient-based descriptors\n  Structured dataset (like felzenszwalb’s part-based recognition)\n  Deterministic or probabilistic decision-making.\n\n\nConcerning activity recognition, the retrieval process is more likely to be probabilistic, using for example Hidden Markov Models. But deterministic methods (for example using Motion history gradients descriptors) exist too.\n\nObject and activity recognition seemed to me to be the most challenging part of modern computer vision.\n\nI did Project number 3 about variations around Microsoft Shadow draw project with Mark Luffel in order to reduce memory load and improve performance for mobility purposes. So we thought about adapting Felzenzwalb’s part-based approach to reduce the size of our dataset. Several hours of brainstorming later and we we’re talking about object recognition instead of patch retrieval. I mentioned that to say that object recognition would be a panacea to many many problems.\n When it didn’t bring a general solution to this challenging problem, CS 7495 had the advantage to explore state-of-the-art research about it and give us the necessary insight to go any further into the subject.\n\n\n\n#####2d / 3d sensors\n\nLectures about stereoscopy, range sensors and the first project I did made me think about the future of sensors for both industrial and general public purposes. The current 3d adaptation of Titanic shows that you don’t really need range sensors to feel the depth, you can actually close one eye without making a big difference. The human brain is able to recognize a rigged body without using range data.\nSo are 3d sensors (stereoscopy) the future of computer vision? I don’t think so. In fact, in high demanding industries, other sensors like LIDARs do a better job. Combining information from accurate range sensors and visual images into RGBZ point clouds (as it has been done with Kinect) could help computer vision to give meaning to images. Since WillowGarage is taking care of the development of PCL (Point Cloud library), we can imagine that kinect-like range sensors will be part of future robot’s battery of sensors. Does range information really helps to understand a picture? For example, it could clearly help segmentation and add constraints to penalty-based probabilistic models. Does the human brain really use 3d information for object recognition for instance? The answer might be ‘no’. Indeed one-eyed persons still perform well at it.\nProject 2 enabled me to spend time on an idea I had about implementing a safe driving app for Iphone including robust detection, features tracing and classifier-based vehicle recognition. Exploring the literature about similar professional projects, I came up with the following question : “Is it more important to refine data accuracy by improving sensors, or to search for new ways of processing data for intelligent and robust detections?”. The arrival on the market of smartphones with poor sensors and decent computational capability would promote the second one, while professional demanding applications will more likely use priceless sensors and focus a bit less on robustness. At the end, robust clever algorithms could support high quality sensors and lead to highly efficient vision systems.\n\n\n\n#####Computer vision, robotics\n\nAs its name implies, Computer Vision focuses on the tiny visible part of the electromagnetic spectrum. Is equipping robots with eyes really necessary? Robots developed for performing industrial specific tasks don’t always need vision. Moreover, lots of systems that use vision use it because it’s the only or cheapest sensor available. So will vision really matter in a future with cheaper and better quality sensors? In fact, CS 7495 made ask myself this question and brought answers too.  Even if lots of papers design ad-hoc solutions for specific issues, general principles emerge from computer vision research about machine understanding, learning and interpreting.\nI’m an aerospace guy, but I felt very comfortable taking this class. Every week it was a great pleasure to listen to Frank Dellaert and all other lecturers. Actually I’d like to combine my engineering skills, my enthusiasm for computer science and fascination for robots by following a PhD in Robots and Intelligent Machines.\n",
        "content": "Being part of the following class projects motivated me to write a couple of thoughts about Computer Vision and my understanding of it coming with an Engineering background. Projects  3D Body Scanner Smartphone app for safe driving Compact database of line drawings Computer science and engineering I always considered Computer Science as a hobby or a technological tool supporting engineering or theoretical research. Back in France, in the so-called Grande Ecoles (engineering schools), the computing department is only here to teach every student basic programming skills. I personally think that every curious and open-minded engineering student should have learned these programming skills by himself years ago. For example, I was surprised to state the small number of aerospace students with a sharp understanding of object-oriented programming. I had a few smartphone apps projects in mind and I wanted to meet people to brainstorm about these projects. That’s one of the reason I wanted to take classes in the College of Computing. I had my first contact with computer vision as an astronomy enthusiast when I had to get clean pictures from noisy low-resolution videos of Saturn. The pipeline was the following: choosing the best images out of one thousand, registering, then stacking them and use wavelets as a final touch. Astronomers are not that good in computer vision. Most of them use opaque processing software and sleep during the processing. Before I entered Georgia Tech, I had a very interesting research project about detecting light flashes caused by asteroids impacting the moon from very noisy and dark videos. So my first job was to register the video. I thought it would be as easy as it was with pictures of planets. But cross-correlation based techniques just didn’t work. That’s why I came to read advanced computer vision literature. I couldn’t imagine that so much had been done about image registration. That’s one of the reasons that pushed me to follow CS7495 this semester. Moreover, I thought it would be awesome to combine two of my favorite subjects: Mathematics and Computer science. I was still thinking I had a good computing background before I started to talk with my classmates. SIFT, SURF, RANSAC, SVM, HOG, all these things sounded like chinese to me. It was quite difficult for me to understand the subtleties of the first lectures, but I caught up quite fast and got fascinated by lots of different topics. Computers vs Humans Before taking this class, it wasn’t clear to me why/when or how the human and the computer could have different approaches to solve a computer vision problem. In the cases where the computer approach was totally different from the human one, was it because of a lack of resources, learning capacity… or was it simply a more judicious choice? I used to consider detection methods that aren’t available to humans (because of computational load for example) as cheating. I always thought there should be some easier method… But reading one of the chapter of Forsyth and Ponce – Computer Vision about Gestalt principles of perceptions made me realize that there were too many processes and resources in human’s brain to be able (nowadays) to identify them and implement them properly. Things like image segmentation look like basic tasks for us, we don’t even think about it. Probabilistic vs Deterministic My first project took advantage of the Kinect 3d sensor to perform unsupervised 3d body scans. One of the most challenging part was non-rigid registration of 3d point clouds. One of the papers I read about it was Anguelov’s “The correlated correspondence algorithm for unsupervised registration of nonrigid surface”. It was the first time I read about bayesian probabilities applied to computer vision problems. I took me a couple of days to be able to understand it completely but it was worth a shot. It led me to understand MRFs and explore belief propagation algorithms and more generally structured model-based methods. Before that I was mostly used to deterministic methods. Recognition paradigm Object recognition is one of these computer vision problems where the human is still performing much better than the machine. State-of-the art research seems to tend to the following paradigm:  Gradient-based descriptors Structured dataset (like felzenszwalb’s part-based recognition) Deterministic or probabilistic decision-making. Concerning activity recognition, the retrieval process is more likely to be probabilistic, using for example Hidden Markov Models. But deterministic methods (for example using Motion history gradients descriptors) exist too. Object and activity recognition seemed to me to be the most challenging part of modern computer vision. I did Project number 3 about variations around Microsoft Shadow draw project with Mark Luffel in order to reduce memory load and improve performance for mobility purposes. So we thought about adapting Felzenzwalb’s part-based approach to reduce the size of our dataset. Several hours of brainstorming later and we we’re talking about object recognition instead of patch retrieval. I mentioned that to say that object recognition would be a panacea to many many problems. When it didn’t bring a general solution to this challenging problem, CS 7495 had the advantage to explore state-of-the-art research about it and give us the necessary insight to go any further into the subject. #####2d / 3d sensors Lectures about stereoscopy, range sensors and the first project I did made me think about the future of sensors for both industrial and general public purposes. The current 3d adaptation of Titanic shows that you don’t really need range sensors to feel the depth, you can actually close one eye without making a big difference. The human brain is able to recognize a rigged body without using range data. So are 3d sensors (stereoscopy) the future of computer vision? I don’t think so. In fact, in high demanding industries, other sensors like LIDARs do a better job. Combining information from accurate range sensors and visual images into RGBZ point clouds (as it has been done with Kinect) could help computer vision to give meaning to images. Since WillowGarage is taking care of the development of PCL (Point Cloud library), we can imagine that kinect-like range sensors will be part of future robot’s battery of sensors. Does range information really helps to understand a picture? For example, it could clearly help segmentation and add constraints to penalty-based probabilistic models. Does the human brain really use 3d information for object recognition for instance? The answer might be ‘no’. Indeed one-eyed persons still perform well at it. Project 2 enabled me to spend time on an idea I had about implementing a safe driving app for Iphone including robust detection, features tracing and classifier-based vehicle recognition. Exploring the literature about similar professional projects, I came up with the following question : “Is it more important to refine data accuracy by improving sensors, or to search for new ways of processing data for intelligent and robust detections?”. The arrival on the market of smartphones with poor sensors and decent computational capability would promote the second one, while professional demanding applications will more likely use priceless sensors and focus a bit less on robustness. At the end, robust clever algorithms could support high quality sensors and lead to highly efficient vision systems. #####Computer vision, robotics As its name implies, Computer Vision focuses on the tiny visible part of the electromagnetic spectrum. Is equipping robots with eyes really necessary? Robots developed for performing industrial specific tasks don’t always need vision. Moreover, lots of systems that use vision use it because it’s the only or cheapest sensor available. So will vision really matter in a future with cheaper and better quality sensors? In fact, CS 7495 made ask myself this question and brought answers too. Even if lots of papers design ad-hoc solutions for specific issues, general principles emerge from computer vision research about machine understanding, learning and interpreting. I’m an aerospace guy, but I felt very comfortable taking this class. Every week it was a great pleasure to listen to Frank Dellaert and all other lecturers. Actually I’d like to combine my engineering skills, my enthusiasm for computer science and fascination for robots by following a PhD in Robots and Intelligent Machines. ",
        "url": "/2012/03/24/computer-vision-report/"
      },
    
      {
        "title": "Automated grading of cancerous tissue samples",
        "excerpt": "This a follow-up paper of the Biomedical Pattern Recognition class I took out of curiosity. It turned out to be very interesting and I ended up helping the Georgia Tech Bio-Medical Informatics and Bioimaging Lab with some of the Machine Learning issues.\n\n\n",
        "content": "This a follow-up paper of the Biomedical Pattern Recognition class I took out of curiosity. It turned out to be very interesting and I ended up helping the Georgia Tech Bio-Medical Informatics and Bioimaging Lab with some of the Machine Learning issues. ",
        "url": "/2012/03/25/grading-cancerous-tissue/"
      },
    
      {
        "title": "Chasing exoplanets from your backyard",
        "excerpt": "The full original report is available here in french. This post gives a short english summary of our adventure, it is still in progress.\n\nIntroduction\nI often refer to this amazing underrated movie - The Truman Show - that I discovered thanks to my highschool English teacher a couple of years back: even if there seems to be a consensus among a community, it doesn’t hurt to experience the truth yourself. It is actually a very important step in research: one should always try to reproduce previous findings as no one is fully immune to honest mistakes.\n\nA simple “Initiation to research” undergrad project.\nIn 2010, Romain Pennec and I had to find a topic for our Supaero (French aerospace school) 1st year research project. It had to be related to our common passion for astronomy. It also had to be mind-blowing. It had come to our ears that a very skillfull pseudo-professional french astronomer - Christian Buil - had managed to confirm the existence of an extrasolar planet from his backyard - using a spectrometer -. Although we only had a couple of years of experience with telescopes, we had the math, the energy and motivation on our side so we decided to commit to such an adventure.\n\nThe transit method\nWe quickly came to the conclusion that the only affordable method would be the transit method - which, similarly to an eclipse, consists in observing the drop of brightness of a remote star when its hypothetical planet transits between its host star and our point of view (earth) - also, our time horizon for this project forbid any long term observation and therefore we had to give up on discovering a new exoplanet and rather focus on confirming a previous finding.\n\n\nThe target: HD189733b\nBy the end of 2009, only 72 extrasolar planets had been discovered using the transit method. We used the excellent resource http://var2.astro.cz to find the most promising target with respect to our time frame and available instruments. It turned out HD189733 b was to transit in front of its host star HD189733 three times during our project, and two occurrences were potentially observable from South of France. Other parameters were to be considered such as the altitude of the star - how high it is in the sky, the higher the thiner of an atmospheric layer the light has to go through - the azimuth of the star from your observation point - this matters most in suburban areas with light pollution being more important in the direction of big cities, if you have a car and a mobile setup then this should not be as important.  , the apparent magnitude - the brightness of a star given in a logarithmic scale - and the predicted depth of the drop of brightness provoked by the transit were factors that lead us pick HD189733 b as our target.\nHD189733 (to not mix up with its hypothetical exoplanet HD189733 b) is a quite close (63 light years away) orange dwarf star you can observe in the Vulpecula - little fox - constellation. Its average apparent magnitude is 7.33, which means it is 610 times darker than Vega, one of the brightest stars of the northern hemisphere. It is also slightly darker than the darkest star the human eye could capture in perfect conditions which means that the first challenge of our experiment will be to find HD189733 in the night sky !!\nThe first observation night takes place on campus in Toulouse, with a terrible light pollution. We aligned our telescope with respect to the polar star, and used the 3-star alignment method of our motorized mount to be able to photograph the wished area. (With such a poor sky, it is virtually impossible to try to find such a star manually). It is still hard to know whether we are looking at the right place as we can’t see the nearby nebulae through the ocular lens as we thought we would. Indeed, the Dumbbell nebulae (M27) is not far as you will see in the following pictures. We took a couple of shots and after some basic image processing, we found out we were almost at the right place!\n\nThe blue image represents the area we shot and the dark overlay was taken from a sky map to get an understanding of where we should have been looking at. Ok we missed HD189733 for a couple of pixels but we knew we could at least somewhat trust our alignment and pointing method.\n\nMeasuring our instrumental accuracy\nWe had read a couple of stories and felt confident our equipment was theoretically good enough to capture the drop of photons caused by the transit of HD189733b. However many sources of disturbances can decrease our signal-noise ratio and therefore we had to check how accurate our measurement pipeline could be. We drove an hour away from Toulouse to get a better sky and simply did dozens of 30s shots of HD189733’s area.\n\nThanks to our previous experience, find the star was much easier and using the technique of differential photometry with three neighbor stars, we were able to find our measurement precision. After a thorough image processing pipleine (using darks, flats, ..), we obtained a standard deviation of 7 mmag which was great news as the depth of the transit was supposed to be around 30 mmag.\n\n\nShoot it\nComing soon ;)\n\nAnalyze the data\nComing soon ;)\n\nSources:\n\n  http://www.astrosurf.com/~buil/extrasolar/obs.htm\n\n",
        "content": "The full original report is available here in french. This post gives a short english summary of our adventure, it is still in progress. Introduction I often refer to this amazing underrated movie - The Truman Show - that I discovered thanks to my highschool English teacher a couple of years back: even if there seems to be a consensus among a community, it doesn’t hurt to experience the truth yourself. It is actually a very important step in research: one should always try to reproduce previous findings as no one is fully immune to honest mistakes. A simple “Initiation to research” undergrad project. In 2010, Romain Pennec and I had to find a topic for our Supaero (French aerospace school) 1st year research project. It had to be related to our common passion for astronomy. It also had to be mind-blowing. It had come to our ears that a very skillfull pseudo-professional french astronomer - Christian Buil - had managed to confirm the existence of an extrasolar planet from his backyard - using a spectrometer -. Although we only had a couple of years of experience with telescopes, we had the math, the energy and motivation on our side so we decided to commit to such an adventure. The transit method We quickly came to the conclusion that the only affordable method would be the transit method - which, similarly to an eclipse, consists in observing the drop of brightness of a remote star when its hypothetical planet transits between its host star and our point of view (earth) - also, our time horizon for this project forbid any long term observation and therefore we had to give up on discovering a new exoplanet and rather focus on confirming a previous finding. The target: HD189733b By the end of 2009, only 72 extrasolar planets had been discovered using the transit method. We used the excellent resource http://var2.astro.cz to find the most promising target with respect to our time frame and available instruments. It turned out HD189733 b was to transit in front of its host star HD189733 three times during our project, and two occurrences were potentially observable from South of France. Other parameters were to be considered such as the altitude of the star - how high it is in the sky, the higher the thiner of an atmospheric layer the light has to go through - the azimuth of the star from your observation point - this matters most in suburban areas with light pollution being more important in the direction of big cities, if you have a car and a mobile setup then this should not be as important. , the apparent magnitude - the brightness of a star given in a logarithmic scale - and the predicted depth of the drop of brightness provoked by the transit were factors that lead us pick HD189733 b as our target. HD189733 (to not mix up with its hypothetical exoplanet HD189733 b) is a quite close (63 light years away) orange dwarf star you can observe in the Vulpecula - little fox - constellation. Its average apparent magnitude is 7.33, which means it is 610 times darker than Vega, one of the brightest stars of the northern hemisphere. It is also slightly darker than the darkest star the human eye could capture in perfect conditions which means that the first challenge of our experiment will be to find HD189733 in the night sky !! The first observation night takes place on campus in Toulouse, with a terrible light pollution. We aligned our telescope with respect to the polar star, and used the 3-star alignment method of our motorized mount to be able to photograph the wished area. (With such a poor sky, it is virtually impossible to try to find such a star manually). It is still hard to know whether we are looking at the right place as we can’t see the nearby nebulae through the ocular lens as we thought we would. Indeed, the Dumbbell nebulae (M27) is not far as you will see in the following pictures. We took a couple of shots and after some basic image processing, we found out we were almost at the right place! The blue image represents the area we shot and the dark overlay was taken from a sky map to get an understanding of where we should have been looking at. Ok we missed HD189733 for a couple of pixels but we knew we could at least somewhat trust our alignment and pointing method. Measuring our instrumental accuracy We had read a couple of stories and felt confident our equipment was theoretically good enough to capture the drop of photons caused by the transit of HD189733b. However many sources of disturbances can decrease our signal-noise ratio and therefore we had to check how accurate our measurement pipeline could be. We drove an hour away from Toulouse to get a better sky and simply did dozens of 30s shots of HD189733’s area. Thanks to our previous experience, find the star was much easier and using the technique of differential photometry with three neighbor stars, we were able to find our measurement precision. After a thorough image processing pipleine (using darks, flats, ..), we obtained a standard deviation of 7 mmag which was great news as the depth of the transit was supposed to be around 30 mmag. Shoot it Coming soon ;) Analyze the data Coming soon ;) Sources: http://www.astrosurf.com/~buil/extrasolar/obs.htm ",
        "url": "/2012/05/15/chasing-exoplanets/"
      },
    
      {
        "title": "Road line detection using OpenCV",
        "excerpt": "This is the report of a Computer Vision mini-project. My objective was to reliably detect lines from dashcam videos. It was a great opportunity to hack around with the great computer vision library OpenCV.\n\n\n",
        "content": "This is the report of a Computer Vision mini-project. My objective was to reliably detect lines from dashcam videos. It was a great opportunity to hack around with the great computer vision library OpenCV. ",
        "url": "/2012/05/25/road-line-detection-using-opencv/"
      },
    
      {
        "title": "Situation Awareness in Rapidly Changing Environments",
        "excerpt": "A quick human-in-the-loop experiment about mental models and situation awareness.\n",
        "content": "This project aims to answer some general questions about the evolution of mental representations of systems and their environment using statistical methods. We used mini-games to design a human experiment and evaluate hypotheses about the evolution of mental models and the impact of workload on situation awareness. The statistical analysis proved several significant interactions between workload and situation awareness and meaningful insight regarding the alteration of mental models. We believe this work can provide a solid experimental basis to support the development of more realistic human agent models. ",
        "url": "/2012/05/25/sa-mental-models/"
      },
    
      {
        "title": "Biologically Inspired Object Recognition",
        "excerpt": "Back in 2013, when I ventured in the Computational Neuroscience class surrounded by BMED students ;)\n",
        "content": "Project description: Coming soon ",
        "url": "/2013/01/25/biologically-inspired-object-recognition/"
      },
    
      {
        "title": "Optimizing with discontinuous cost functions: AI for the final frontier",
        "excerpt": "A fight between Simulated Annealing and Particle Swarm Optimization.\nThe underlying algorithm is now powering the AI of the iOS game I created Astro Fight that was recently taken of the App Store (Damn developer fee ;) However, I’m working on open sourcing it !\n\nHere is a screenshot and the project report in question below.\n\n\n\n\n",
        "content": "A fight between Simulated Annealing and Particle Swarm Optimization. The underlying algorithm is now powering the AI of the iOS game I created Astro Fight that was recently taken of the App Store (Damn developer fee ;) However, I’m working on open sourcing it ! Here is a screenshot and the project report in question below.  ",
        "url": "/2013/02/25/optimizing-final-frontier/"
      },
    
      {
        "title": "Collaborative RRT-based motion planning",
        "excerpt": "Computing collision-free paths for multi-robot environments is a fundamental issue in the world of robotic path planning. Rapidly exploring Random Trees (RRTs) have been proven to be a very powerful method to design quick and efficient single-query path planners. Unfortunately, RRTs struggle with multi-robot worlds due to the explosion of the degrees of freedom and the increase in complexity for collision avoidance between moving agents in close proximity. In this project,we present a method that revolves around the idea of collaborative RRT (cRRT) search that shares information between all the robots in the environment. Planning by combining all the robots into a single high-dimensional RRT many times is unable to compute a collision-free path in a reasonable amount of time or tree nodes. Our method however, takes advantage of the power of RRTs on lower dimensional configurations and collaborates searches between all robots in order to compute collision- free paths for all.\n\nFull-report below:\n\n\n",
        "content": "Computing collision-free paths for multi-robot environments is a fundamental issue in the world of robotic path planning. Rapidly exploring Random Trees (RRTs) have been proven to be a very powerful method to design quick and efficient single-query path planners. Unfortunately, RRTs struggle with multi-robot worlds due to the explosion of the degrees of freedom and the increase in complexity for collision avoidance between moving agents in close proximity. In this project,we present a method that revolves around the idea of collaborative RRT (cRRT) search that shares information between all the robots in the environment. Planning by combining all the robots into a single high-dimensional RRT many times is unable to compute a collision-free path in a reasonable amount of time or tree nodes. Our method however, takes advantage of the power of RRTs on lower dimensional configurations and collaborates searches between all robots in order to compute collision- free paths for all. Full-report below: ",
        "url": "/2013/05/20/collaborative-rtt-based-motion-planning/"
      },
    
      {
        "title": "Proving LQG controller global optimality",
        "excerpt": "When your robust control professor asks you to debunk LQG optimality proofs…\n",
        "content": " ",
        "url": "/2013/05/25/lqg-global-optimality/"
      },
    
      {
        "title": "Optimal dynamic soaring trajectories for a glider",
        "excerpt": "An optimal control project about optimal soaring trajectories.\n",
        "content": "Description: Coming soon ",
        "url": "/2013/05/25/optimal-soaring-glider/"
      },
    
      {
        "title": "Cognitive engineering analysis of an automated car",
        "excerpt": "Here I summarize the final project I did together with Gabriel Gelman for the graduate class Cognitive Engineering taught by Dr Karen Feigh, Professer at Georgia Tech School of Aerospace. The idea was to apply several concepts we learned about work domain analysis, levels of automation and function allocation to the rising problem of self-driving cars. Let’s try to keep in mind that the final report was written in 2011 when Google and other companies’ efforts to create autonomous vehicles were still at the embryonic stage. We emphasize the need for a formal analysis of allocation of tasks between the driver and the autopilot in the transition phase which will take place in the upcoming year: neither the infrastructure nor the technology allows fully-automated vehicles to safely operator on every type of road. Although this project doesn’t attempt to tackle the technical challenges inherent to such automation, it’s still a good read as I believe cognitive aspects should have a more central position in the current debate about autonomous cars.\n\n\n",
        "content": "Here I summarize the final project I did together with Gabriel Gelman for the graduate class Cognitive Engineering taught by Dr Karen Feigh, Professer at Georgia Tech School of Aerospace. The idea was to apply several concepts we learned about work domain analysis, levels of automation and function allocation to the rising problem of self-driving cars. Let’s try to keep in mind that the final report was written in 2011 when Google and other companies’ efforts to create autonomous vehicles were still at the embryonic stage. We emphasize the need for a formal analysis of allocation of tasks between the driver and the autopilot in the transition phase which will take place in the upcoming year: neither the infrastructure nor the technology allows fully-automated vehicles to safely operator on every type of road. Although this project doesn’t attempt to tackle the technical challenges inherent to such automation, it’s still a good read as I believe cognitive aspects should have a more central position in the current debate about autonomous cars. ",
        "url": "/2014/05/25/work-analysis-automated-car/"
      },
    
      {
        "title": "Continuous Kalman Filter Scheduling for Situation Awareness in the Cockpit",
        "excerpt": "Ongoing research in Cognitive Engineering proposes to model an ideal pilot as an optimal state estimator. Control theory tackled the problem of optimally scheduling the allocation of sensors to track multiple correlated targets, using results from operations research. Combining the findings of both disciplines could help with providing a quantitative indicator for best-case performance of the flight crew as a result of the interaction of the aircraft/auto-flight system dynamics, physiological constraints, cockpit interfaces and pilot monitoring patterns. This project investigates the addition of realistic human-related constraints derived from experimental pilot studies and geometrical constraints on the cost function used in the Kalman filter scheduling problem.\n\n\n",
        "content": "Ongoing research in Cognitive Engineering proposes to model an ideal pilot as an optimal state estimator. Control theory tackled the problem of optimally scheduling the allocation of sensors to track multiple correlated targets, using results from operations research. Combining the findings of both disciplines could help with providing a quantitative indicator for best-case performance of the flight crew as a result of the interaction of the aircraft/auto-flight system dynamics, physiological constraints, cockpit interfaces and pilot monitoring patterns. This project investigates the addition of realistic human-related constraints derived from experimental pilot studies and geometrical constraints on the cost function used in the Kalman filter scheduling problem. ",
        "url": "/general/2015/05/26/untitled-continuous-kalman-filter-scheduling-for-situation-awareness-in-the-cockpit/"
      },
    
      {
        "title": "Hack4Europe - VRide",
        "excerpt": "Hacked in 3 week ends, I coded VRide together with Gregoire Deprez, the goal was to enhance Uber/Lift rides with Augmented Reality experiences. More like a proof of concept, it got use familiarized with Vuforia, AR markers, OpenGL, WebGL, CSS3D and Uber APIs.\n\n\n\nThe video is slightly laggy (available hardware was slighty insufficient, we should try again with newer smartphones!), but gives a good impression of the concept !\n\n\n",
        "content": "Hacked in 3 week ends, I coded VRide together with Gregoire Deprez, the goal was to enhance Uber/Lift rides with Augmented Reality experiences. More like a proof of concept, it got use familiarized with Vuforia, AR markers, OpenGL, WebGL, CSS3D and Uber APIs. The video is slightly laggy (available hardware was slighty insufficient, we should try again with newer smartphones!), but gives a good impression of the concept ! ",
        "url": "/2015/05/26/hack4europe-vride/"
      },
    
      {
        "title": "Projects",
        "excerpt": "Academic projects\n\n2016\n\nNetwork security and domain blacklists\n",
        "content": "Academic projects 2016 Network security and domain blacklists ",
        "url": "/2016/03/06/projects/"
      },
    
      {
        "title": "Using Rosbridge and Roslib JS",
        "excerpt": "ROS\nThe Robot Operating system (ROS) was originally developped by Willow Garage …\n\nWhy Rosbridge ?\nROS is already a versatile piece of software offering a native C++ library as well as extensive bindings for python. However, other scripting/programming languages cannot directly benefit from the implementation of the ROS communication protocol as it is the case for Javascript or Java. Rosbridge was developed to fill that gap.\n\n####Pros\n\n  Uses generic network protocols (TCP, Websockets)\n  Connects web interfaces to the ROS network.\n  Offers throttling, queueing optimizations.\n####Cons\n  Breaks ROS’ decentralized network architecture.\n\n\nRoslib JS\n\nvar ros = new ROSLIB.Ros({\n url: 'ws://localhost:9090'\n})\n\n\n",
        "content": "ROS The Robot Operating system (ROS) was originally developped by Willow Garage … Why Rosbridge ? ROS is already a versatile piece of software offering a native C++ library as well as extensive bindings for python. However, other scripting/programming languages cannot directly benefit from the implementation of the ROS communication protocol as it is the case for Javascript or Java. Rosbridge was developed to fill that gap. ####Pros Uses generic network protocols (TCP, Websockets) Connects web interfaces to the ROS network. Offers throttling, queueing optimizations. ####Cons Breaks ROS’ decentralized network architecture. Roslib JS var ros = new ROSLIB.Ros({ url: 'ws://localhost:9090' }) ",
        "url": "/2016/03/06/understanding-rosbridge-and-roslib-js/"
      },
    
      {
        "title": "Embed documents in your Ghost blog using ViewerJS and Express",
        "excerpt": "Introduction\n\nWhether you are using Ghost as a blogging platform or to host your CV and research projects, it could come handy to embed documents such as PDF presentations or spreadsheets in a post. Embedding documents will  ensure that your readers don’t have to download anything which will make their experience smoother.\n\nViewerJS\n\nViewerJs is a javascript library making it easy to embed self-hosted documents in pages of your website. It uses Mozilla’s well know PDF.js and KO’s WebODF under the hood. A ViewerJS plug-in is available for Wordpress but no such thing seems to exist to ease integration for Ghost bloggers.\n\nGhost and files\n\nGhost doesn’t host documents for you as it does with post images. So we will have to setup our own simple file server. We will achieve this using the famous Express module for NodeJS.\n#####Setting up a minimalistic file server\n\nFirst your will need to download ViewerJS from their GetIt page. From a Linux-based server, you can use the command line:\nwget http://viewerjs.org/releases/ViewerJS-latest.zip\nunzip ViewerJS-latest.zip  -d ViewerJS\n\n\n\nCreate a folder somewhere in your server and, and copy the ViewerJS folder inside.\nmkdir docServer\ncp -R ~/Downloads/ViewerJS/viewerjs-0.5.8/ViewerJS ./docServer/\ncd docServer\n\n\n\nNow that we are in docServer we can install the NodeJS module Express which will serve the ViewerJS library and the embedded documents.\nnpm install express\n\n\n\nFinally we create a server.js file in which we put the minimalistic server code. (We picked 2375 but you can use any available port you want - preferably above 1024 so that you don’t need sudo privileges to run it).\nvar express = require('express');\nvar app = express();\napp.use('/ViewerJS', express.static(__dirname + '/ViewerJS'));\napp.use('/docs', express.static(__dirname + '/data'));\nvar server = app.listen(2375);\n\n\n\nLet’s now launch the server (We assume that you use PM2 to run Ghost on your production server - if not you should seriously think about it-.)\npm2 start server.js --name docServer\n\n\n\nEven better on port 80\nWe assume that you have a Ghost blog proxied through a web server such as Nginx or Apache and that you have a certain level of privileges allowing to modify virtual host configuration files.\n\nApache\nModify your existing Ghost virtualhost to make it look like that:\n &lt;VirtualHost *:80&gt;\n     ServerName yourghostblog.com\n     ProxyPreserveHost on\n     ProxyPass /docs/ http://localhost:2375/docs/\n     ProxyPass /ViewerJS/ http://localhost:2375/ViewerJS/\n     ProxyPass / http://localhost:2368/\n&lt;/VirtualHost&gt;\n\n\n\nNginx\nToDo\n\nSynchronizing files\nIt is quite annoying to upload documents through ssh. To reduce the pain,you can use Rsync (On Mac and Linux) to synchronize you documents between you local environment and the server host the Ghost blog.\n\n##Conclusion\nHere we go, you can now include embedded documents using ViewerJS instructions, i.e in a Ghost blog post, just use the following HTML\n&lt;iframe src = \"/ViewerJS/#/docs/path/to/your/doc.pdf\" width='100%' height='600' allowfullscreen webkitallowfullscreen&gt;&lt;/iframe&gt; \n\n\n\n\n\n\n  It appears you don't have a PDF plugin for this browser.\n  No biggie... you can click here to\n  download the PDF file.\n\n",
        "content": "Introduction Whether you are using Ghost as a blogging platform or to host your CV and research projects, it could come handy to embed documents such as PDF presentations or spreadsheets in a post. Embedding documents will ensure that your readers don’t have to download anything which will make their experience smoother. ViewerJS ViewerJs is a javascript library making it easy to embed self-hosted documents in pages of your website. It uses Mozilla’s well know PDF.js and KO’s WebODF under the hood. A ViewerJS plug-in is available for Wordpress but no such thing seems to exist to ease integration for Ghost bloggers. Ghost and files Ghost doesn’t host documents for you as it does with post images. So we will have to setup our own simple file server. We will achieve this using the famous Express module for NodeJS. #####Setting up a minimalistic file server First your will need to download ViewerJS from their GetIt page. From a Linux-based server, you can use the command line: wget http://viewerjs.org/releases/ViewerJS-latest.zip unzip ViewerJS-latest.zip -d ViewerJS Create a folder somewhere in your server and, and copy the ViewerJS folder inside. mkdir docServer cp -R ~/Downloads/ViewerJS/viewerjs-0.5.8/ViewerJS ./docServer/ cd docServer Now that we are in docServer we can install the NodeJS module Express which will serve the ViewerJS library and the embedded documents. npm install express Finally we create a server.js file in which we put the minimalistic server code. (We picked 2375 but you can use any available port you want - preferably above 1024 so that you don’t need sudo privileges to run it). var express = require('express'); var app = express(); app.use('/ViewerJS', express.static(__dirname + '/ViewerJS')); app.use('/docs', express.static(__dirname + '/data')); var server = app.listen(2375); Let’s now launch the server (We assume that you use PM2 to run Ghost on your production server - if not you should seriously think about it-.) pm2 start server.js --name docServer Even better on port 80 We assume that you have a Ghost blog proxied through a web server such as Nginx or Apache and that you have a certain level of privileges allowing to modify virtual host configuration files. Apache Modify your existing Ghost virtualhost to make it look like that: &lt;VirtualHost *:80&gt; ServerName yourghostblog.com ProxyPreserveHost on ProxyPass /docs/ http://localhost:2375/docs/ ProxyPass /ViewerJS/ http://localhost:2375/ViewerJS/ ProxyPass / http://localhost:2368/ &lt;/VirtualHost&gt; Nginx ToDo Synchronizing files It is quite annoying to upload documents through ssh. To reduce the pain,you can use Rsync (On Mac and Linux) to synchronize you documents between you local environment and the server host the Ghost blog. ##Conclusion Here we go, you can now include embedded documents using ViewerJS instructions, i.e in a Ghost blog post, just use the following HTML &lt;iframe src = \"/ViewerJS/#/docs/path/to/your/doc.pdf\" width='100%' height='600' allowfullscreen webkitallowfullscreen&gt;&lt;/iframe&gt;  It appears you don't have a PDF plugin for this browser. No biggie... you can click here to download the PDF file. ",
        "url": "/2016/03/06/embed-documents-in-your-ghost-blog-using-viewerjs/"
      },
    
      {
        "title": "Optimizing the Dell XPS 15 9550 for Ubuntu 16.04 / 16.10",
        "excerpt": "This laptop is a combination of beauty and performance that many developers will appreciate. Therefore Windows is not alway the best choice and one might greatly benefit from a dual boot setting. This article describes how to setup efficiently Ubuntu 16.04 on the XPS 15 9550.\n\nInstallation\nI followed the steps shown on this page and it seemed to work quite well. I suggest to use Rufus to create the Ubuntu bootable USB disk on Windows.\nI just had to launch the Live CD first and then install from there (The direct install option was buggy somehow).\nYou get something quite convincing out of the box, but there is still some work to get everything work perfectly.\n\nFreezes on Windows (dual boot)\nAfter switching from Raid to AHCI - as suggested in this tutorial - you may encounter BSOD freezes - with the only hint being CRITICAL_PROCESS_DIED. It seems to be due to the SSD driver, installing the last Samsung 950 pro drivers solved the problem for me.\n\nHiDpi\nFor 4K (HiDpi) screens, Gnome allows you to have everything scaled up (same as in windows 10). For this just go in System settings &gt; Display and use the Scale for menu and bars slider to something like 2.25. This was good for me. Qt-based apps might not use this settings. (Example: Qt Creator / Tex Studio, …)\n\nFor QT5 applications, ArchiWiki’s HiDpi page rightfully suggests to create the file /etc/profile.d/qt-hidpi.sh, give it execution permission (sudo chmod +x /etc/profile.d/qt-hidpi.sh). You will need to restart Ubuntu to account for this change.\n\nMulti-screen setup\nI found it very painful to independently setup HiDpi settings for multiple screens. (For instance, one 1080p external monitor along with your 4k laptop screen). I did not find any solution using the Nvidia drivers (xrandr crashes), and the following setup - from here - works when switching over to Intel (using nvidia-settings).\nxrandr --output eDP-1 --auto --output HDMI-1 --auto --panning 3840x2160+3840+0 --scale 2x2 --right-of eDP-1\n\n\nThis will allow you to have your external monitor on the right of your 4k laptop. Make sure to increase the Scale in Ubuntu’s Displays GUI as well.\n\nGraphics\nNvidia drivers for Linux seemed pretty unstabled on the XPS until I tried version 375.xx which seems to work very well.\n\nFirst, add the ppa containing up-to-date packages:\nsudo add-apt-repository ppa:graphics-drivers/ppa\nsudo apt update\n\n\nThen you can install the drivers using\nsudo apt-get install nvidia-375\n\n\nYou can switch back and forth between different graphics drivers using the Additional drivers Gnome GUI.\n\nBluetooth\nOut of the box, bluetooth can’t find any device. Following this dark magic steps (from the ubuntuforum thread) fixed it for me, but I would recommend you to investigate this before blindly apply this fix.\n\n\n  Download the firmware from an obscure dropbox https://www.dropbox.com/s/8goc4omhnzxij93/BCM-0a5c-6410.hcd?dl=0\n  sudo cp BCM-0a5c-6410.hcd /lib/firmware/brcm/\n  Reboot\n\n\nFixing Palm detection (tested on Ubuntu 16.04)\nSomething super annoying that happens when typing is that the palm of your hand accidently taps the touchpad. This has the undesirable effect of jumping the cursor to wherever the mouse is, selecting random chunks of texts and messing up with your input. It took me a while to find a proper solution, this seems to be working - from [here](http://wiki.yobi.be/wiki/Laptop_Dell_XPS_15](http://wiki.yobi.be/wiki/Laptop_Dell_XPS_15).\n\nAdd to the file /etc/modprobe.d/blacklist.conf the following line:\nblacklist i2c_designware-platform\n\n\nReboot your linux system.\n\nEnergy comsumption\nThe battery lasts longer on Windows than Linux, so there is a lot of room for improvement on this aspect. It may be due to Skylake processors power management which still needs some tuning on Linux. The first thing to do to save battery is to use the Intel GPU (for this install nvidia-prime, then the program Nvidia Settings will allow you to switch). I run some battery tests using powertop and the best I can get with Ubuntu (With the 4K model) is 10.5W when idle with screen brightness on 20%, 12W with Wifi/Firefox on. About twice as much when using Nvidia GPU. I tested Linux kernel 4.4.2 and found no difference with 4.4.0 present in Xenial by default. I’ll keep investigating this as it appears Windows manages to only use 5W when idle.\n\n(Update May 2016) Testing Kernel 4.6\nLinux Kernel 4.6 got released seemingly with improvements regarding Skylake architectures. Let’s test it !\nIt is preferable to switch from Nvidia drivers to the free driver before install the new kernel (Got a bunch of errors before I switched), you can do that in the Ubuntu Additional drivers utility program. Unfortunately, energy consumption did not improve, however, I feel startup and shutdown times improved and dual screen setup is much more stable! (No more freezes).\nAlso, a very annoying bug was making any chromium-based IDE (Atom, Visual Studio Code) very laggy using Intel graphics, this seems to be fixed!\n\n(Update Feb 2017) Ubuntu 16.10 Yakety Yak\nLast Ubuntu update installs Kernel 4.8 which can only be good for your Skylake CPU. Did not have time for a quantitative analysis yet, but the system is stable.\n\nSources\n\n\n  http://wiki.yobi.be/wiki/Laptop_Dell_XPS_15\n  http://ubuntuforums.org/showthread.php?t=2317843\n\n",
        "content": "This laptop is a combination of beauty and performance that many developers will appreciate. Therefore Windows is not alway the best choice and one might greatly benefit from a dual boot setting. This article describes how to setup efficiently Ubuntu 16.04 on the XPS 15 9550. Installation I followed the steps shown on this page and it seemed to work quite well. I suggest to use Rufus to create the Ubuntu bootable USB disk on Windows. I just had to launch the Live CD first and then install from there (The direct install option was buggy somehow). You get something quite convincing out of the box, but there is still some work to get everything work perfectly. Freezes on Windows (dual boot) After switching from Raid to AHCI - as suggested in this tutorial - you may encounter BSOD freezes - with the only hint being CRITICAL_PROCESS_DIED. It seems to be due to the SSD driver, installing the last Samsung 950 pro drivers solved the problem for me. HiDpi For 4K (HiDpi) screens, Gnome allows you to have everything scaled up (same as in windows 10). For this just go in System settings &gt; Display and use the Scale for menu and bars slider to something like 2.25. This was good for me. Qt-based apps might not use this settings. (Example: Qt Creator / Tex Studio, …) For QT5 applications, ArchiWiki’s HiDpi page rightfully suggests to create the file /etc/profile.d/qt-hidpi.sh, give it execution permission (sudo chmod +x /etc/profile.d/qt-hidpi.sh). You will need to restart Ubuntu to account for this change. Multi-screen setup I found it very painful to independently setup HiDpi settings for multiple screens. (For instance, one 1080p external monitor along with your 4k laptop screen). I did not find any solution using the Nvidia drivers (xrandr crashes), and the following setup - from here - works when switching over to Intel (using nvidia-settings). xrandr --output eDP-1 --auto --output HDMI-1 --auto --panning 3840x2160+3840+0 --scale 2x2 --right-of eDP-1 This will allow you to have your external monitor on the right of your 4k laptop. Make sure to increase the Scale in Ubuntu’s Displays GUI as well. Graphics Nvidia drivers for Linux seemed pretty unstabled on the XPS until I tried version 375.xx which seems to work very well. First, add the ppa containing up-to-date packages: sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt update Then you can install the drivers using sudo apt-get install nvidia-375 You can switch back and forth between different graphics drivers using the Additional drivers Gnome GUI. Bluetooth Out of the box, bluetooth can’t find any device. Following this dark magic steps (from the ubuntuforum thread) fixed it for me, but I would recommend you to investigate this before blindly apply this fix.  Download the firmware from an obscure dropbox https://www.dropbox.com/s/8goc4omhnzxij93/BCM-0a5c-6410.hcd?dl=0 sudo cp BCM-0a5c-6410.hcd /lib/firmware/brcm/ Reboot Fixing Palm detection (tested on Ubuntu 16.04) Something super annoying that happens when typing is that the palm of your hand accidently taps the touchpad. This has the undesirable effect of jumping the cursor to wherever the mouse is, selecting random chunks of texts and messing up with your input. It took me a while to find a proper solution, this seems to be working - from [here](http://wiki.yobi.be/wiki/Laptop_Dell_XPS_15](http://wiki.yobi.be/wiki/Laptop_Dell_XPS_15). Add to the file /etc/modprobe.d/blacklist.conf the following line: blacklist i2c_designware-platform Reboot your linux system. Energy comsumption The battery lasts longer on Windows than Linux, so there is a lot of room for improvement on this aspect. It may be due to Skylake processors power management which still needs some tuning on Linux. The first thing to do to save battery is to use the Intel GPU (for this install nvidia-prime, then the program Nvidia Settings will allow you to switch). I run some battery tests using powertop and the best I can get with Ubuntu (With the 4K model) is 10.5W when idle with screen brightness on 20%, 12W with Wifi/Firefox on. About twice as much when using Nvidia GPU. I tested Linux kernel 4.4.2 and found no difference with 4.4.0 present in Xenial by default. I’ll keep investigating this as it appears Windows manages to only use 5W when idle. (Update May 2016) Testing Kernel 4.6 Linux Kernel 4.6 got released seemingly with improvements regarding Skylake architectures. Let’s test it ! It is preferable to switch from Nvidia drivers to the free driver before install the new kernel (Got a bunch of errors before I switched), you can do that in the Ubuntu Additional drivers utility program. Unfortunately, energy consumption did not improve, however, I feel startup and shutdown times improved and dual screen setup is much more stable! (No more freezes). Also, a very annoying bug was making any chromium-based IDE (Atom, Visual Studio Code) very laggy using Intel graphics, this seems to be fixed! (Update Feb 2017) Ubuntu 16.10 Yakety Yak Last Ubuntu update installs Kernel 4.8 which can only be good for your Skylake CPU. Did not have time for a quantitative analysis yet, but the system is stable. Sources  http://wiki.yobi.be/wiki/Laptop_Dell_XPS_15 http://ubuntuforums.org/showthread.php?t=2317843 ",
        "url": "/2016/04/06/dell-xps-15-9550-ubuntu-16-04/"
      },
    
      {
        "title": "HLA Certi, getting started !",
        "excerpt": "HLA stands for High Level Architecture, a set of specifications for distributed simulation systems that originated from the US Department of Defense (DoD) around 1996. (Needs citation)\n\nHLA Overview\n\nConcepts\nToDO\n\nFeatures\n\nCerti\n\nCerti is an open-source implementation of HLA specifications. It provides a HLA Runtime Infrastruture (RTI), a Federation Ambassador module and APIs to build a HLA federation.\n\nCompiling CERTI\n\nPredependencies\nYou will need to install Cmake and have GCC installed on your Linux system. I found it mandatory to install YACC and LEX as well. libxml2-dev is optional, you want to install it to be able to export/import federations.\nsudo apt-get install build-essential cmake sudo byacc flex libxml2-dev\n\n\nBuild Certi from sources\nThe first thing to do to work with Certi is to get the sources from Certi’s git forge.\nThen create a build folder.\n\ngit clone git://git.savannah.nongnu.org/certi.git\ncd certi\nmkdir build\ncd build\n\n\n\nRun Cmake by picking an installation folder installFolder and compile with make. In make -jn, replace n with the number of cores to speed up compilation.\n\ncmake -DCMAKE_ISNTALL_PREFIX=/installFolder ..\nmake -j4\nmake install\n\n\n\nWhen using recent versions of GCC, compilation might fail with the following error message:\n[ 66%] Linking CXX executable TestFedTime\nlibFedTimed.so.1.0.0: undefined reference to `typeinfo for RTI::Exception`\n\n\n\nThis is due to a cyclical dependency (FedTime is supposed to throw RTI::Exception) not declared in CMake (to make it work). However, throwing exceptions requires a typeinfo lookup which triggers GCC’s error. Making RTI::Exception destructor purely virtual seems to fix the problem. (Probably because in the absence of non-inline virtual methods, GCC copies the typeinfo to all relevant translation units, see here.\n\nLast step is to run a script to setup the approriate environment variables.\n\nsource installFolder/myCERTI_env.sh\n\n\n\nI recommand to add this line at the end of your ~/.bashrc file so that you don’t have to rerun it everytime.\n\nGetting started with HLA Certi\n\nNow we want to run the CERTI tutorial.\nFirst, clone the applications repository\n\ngit clone git://git.savannah.nongnu.org/certi/applications.git Certi-Apps\n\n\n\nIt’s now time to build the tutorial:\ncd Certi-Apps/HLA_Tutorial\nmkdir build\ncd build\ncmake -DCMAKE_ISNTALL_PREFIX=appsInstallFolder ..\nmake\nmake install\n\n\n\nNow open three terminals and run respectively:\n\n  cd into appsInstallFolder/share/federations. Launch the Runtime rtig\n  Launch the tutorial controller app appsInstallFolder/bin/controllerFederate\n  Launch the tutorial process app appsInstallFolder/bin/processFederate\n\n\nThen terminal 2 should guide you through the the tutorial, make sure you observe the three terminals outputs during the execution of the tutorial.\n\nSources:\n\n\n  https://gcc.gnu.org/wiki/Visibility\n  https://gcc.gnu.org/onlinedocs/gcc/Vague-Linkage.html\n  http://www.nongnu.org/certi/certi_doc/Install/html/build.html\n\n",
        "content": "HLA stands for High Level Architecture, a set of specifications for distributed simulation systems that originated from the US Department of Defense (DoD) around 1996. (Needs citation) HLA Overview Concepts ToDO Features Certi Certi is an open-source implementation of HLA specifications. It provides a HLA Runtime Infrastruture (RTI), a Federation Ambassador module and APIs to build a HLA federation. Compiling CERTI Predependencies You will need to install Cmake and have GCC installed on your Linux system. I found it mandatory to install YACC and LEX as well. libxml2-dev is optional, you want to install it to be able to export/import federations. sudo apt-get install build-essential cmake sudo byacc flex libxml2-dev Build Certi from sources The first thing to do to work with Certi is to get the sources from Certi’s git forge. Then create a build folder. git clone git://git.savannah.nongnu.org/certi.git cd certi mkdir build cd build Run Cmake by picking an installation folder installFolder and compile with make. In make -jn, replace n with the number of cores to speed up compilation. cmake -DCMAKE_ISNTALL_PREFIX=/installFolder .. make -j4 make install When using recent versions of GCC, compilation might fail with the following error message: [ 66%] Linking CXX executable TestFedTime libFedTimed.so.1.0.0: undefined reference to `typeinfo for RTI::Exception` This is due to a cyclical dependency (FedTime is supposed to throw RTI::Exception) not declared in CMake (to make it work). However, throwing exceptions requires a typeinfo lookup which triggers GCC’s error. Making RTI::Exception destructor purely virtual seems to fix the problem. (Probably because in the absence of non-inline virtual methods, GCC copies the typeinfo to all relevant translation units, see here. Last step is to run a script to setup the approriate environment variables. source installFolder/myCERTI_env.sh I recommand to add this line at the end of your ~/.bashrc file so that you don’t have to rerun it everytime. Getting started with HLA Certi Now we want to run the CERTI tutorial. First, clone the applications repository git clone git://git.savannah.nongnu.org/certi/applications.git Certi-Apps It’s now time to build the tutorial: cd Certi-Apps/HLA_Tutorial mkdir build cd build cmake -DCMAKE_ISNTALL_PREFIX=appsInstallFolder .. make make install Now open three terminals and run respectively: cd into appsInstallFolder/share/federations. Launch the Runtime rtig Launch the tutorial controller app appsInstallFolder/bin/controllerFederate Launch the tutorial process app appsInstallFolder/bin/processFederate Then terminal 2 should guide you through the the tutorial, make sure you observe the three terminals outputs during the execution of the tutorial. Sources:  https://gcc.gnu.org/wiki/Visibility https://gcc.gnu.org/onlinedocs/gcc/Vague-Linkage.html http://www.nongnu.org/certi/certi_doc/Install/html/build.html ",
        "url": "/2016/04/10/hla-certi-getting-started/"
      },
    
      {
        "title": "Sébastien Mamessier",
        "excerpt": "\n\nCV/Resume\nHere you can find my CV in a pdf format.\n&lt;/p&gt;\n\n\n\n&lt;/div&gt;\n\n\n\nI'm currently a Senior Graduate Researcher and Robotics PhD candidate at Georgia Tech School of Aerospace, working on adaptive controls for semi-autonomous cars as well as collaborative artificial intelligence for future commercial aviation cockpits.\n\nDuring my curriculum, I worked for two and a half years at Airbus Group Innovations in Munich, Germany, developing cockpit / flight automation concepts and evaluating them using Virtual Reality experiments. Before that, I received a M.S in Aerospace Engineering from Georgia Tech and a Diplome d'Ingenieur from Supaero (France) in 2013.\n\nMy PhD thesis work focuses on computational modeling of safe and efficient Human-AI collaboration. Potential applications of my research span from flight deck automation to integration of adaptive controls in semi-automated cars.\n\nMy fields of expertise and interests include Human Factors, Controls, Artificial Intelligence and Software Engineering. Private pilot since 2013, I'm a big soccer fan, practice astronomy and contribute to open source projects.\n\n",
        "content": " CV/Resume Here you can find my CV in a pdf format. &lt;/p&gt; &lt;/div&gt; I'm currently a Senior Graduate Researcher and Robotics PhD candidate at Georgia Tech School of Aerospace, working on adaptive controls for semi-autonomous cars as well as collaborative artificial intelligence for future commercial aviation cockpits. During my curriculum, I worked for two and a half years at Airbus Group Innovations in Munich, Germany, developing cockpit / flight automation concepts and evaluating them using Virtual Reality experiments. Before that, I received a M.S in Aerospace Engineering from Georgia Tech and a Diplome d'Ingenieur from Supaero (France) in 2013. My PhD thesis work focuses on computational modeling of safe and efficient Human-AI collaboration. Potential applications of my research span from flight deck automation to integration of adaptive controls in semi-automated cars. My fields of expertise and interests include Human Factors, Controls, Artificial Intelligence and Software Engineering. Private pilot since 2013, I'm a big soccer fan, practice astronomy and contribute to open source projects. ",
        "url": "/2016/05/15/cv/"
      },
    
      {
        "title": "Intuitive vs Nonintuitive decision making",
        "excerpt": "This blog post summarizes Simmons and Nelson’ study about how people incorporate constraining/contradicting information into their initial intuitive thought when asked to make a decision. It was written in the context of the Intro to Cognitive Science class.\n\nIntuitive Confidence: Choosing Between Intuitive and Nonintuitive Alternatives\n\nAuthors: Joseph P. Simmons (Princeton University), Leif D. Nelson (New York University)\n\nIntroduction\n\nPeople seem to favor intuitive options rather than equally or more valid nonintuitive options when taking decisions. This paper intends to explain how people weigh intuitive answers and nonintuitive alternatives that might oppose their initial intuition.\n\nIt has been shown that people often prefer to follow their intuition even when conflicting with other available information, leading to judgment biases. Simmons and Nelson review the relevant related phenomena such as transparency illusions, beliefs in explicitly false statements and other biases appearing at different levels of human cognition. It seems plausible that two distinct systems are competing, the first one being responsible for fast, effortless, heuristics and knowledge based decisions whereas the second system - much slower and resource-demanding - attempts to correct the initial judgment using available cues, rules and reasoning.\n\nSome theories indicate that cognitive lazyness could prevent System 2 from kicking in and contributing to the decision. Others state that the sequential nature of the judgment process advantages the primity of the intuitive thought as System 2 has to falsify System 1’s conclusions persuasively. This phenomena is commonly refered to as anchoring and adjustment.\n\nHowever, such models can’t extensively explain the observed proportion of intuitive biases among motivated reasoners who fully process contradictory information. Simmons and Nelson propose a so-call dual-process model that supposedly explains the observed phenomena and makes relevant predictions.\n\nSimmon and Nelson’s model\n\nThe authors’s model relies on four hypotheses:\n\n1) Intuitions are chosen more often because people hold them with high Confidence\n\n2) The magnitude of an opposing piece of information matters for invalidating intuition.\n\n3) People who are more confident about their intuition will follow them more often\n\n4) People who betray their intuition are less confident with their final choice.\n\nThese rather natural hypotheses are then evaluated by the authors using predition of sporting events as a field case as it provides the experiment with the required variability of inputs magnitude and intuitive confidence.\n\nThe example of the football bookmaker point spread concept is demonstrated, attempting to explain how most people handle this question. In this case, the initial intuition will answer the question which team will win and the point spread serves as the constraining information where further reasoning should be involved. All four hypotheses can be nicely instantiated against this scenario and evaluated against historical data.\n\nThe following bullets summarize the result of Simmon and Nelson’s studies regarding the point spread example:\n\n\n  \n    Most people bet on favorites in 90% of the games despite of the the point spread. Hypothesis 1 is verified (holds for rookies and experts)\n  \n  \n    The spread magnitude negatively impacts this proportion. Hyp 2 is correct (holds for rookies and experts)\n  \n  \n    People betting on underdogs showed less confidence on the outcome, which seems to verify hypothesis 4 is verified.\n  \n  \n    Finally, people bet for favorites even when setting the spread themselves. This study was design to eliminate the hypothesis that people simply don’t understand the point spread concept when betting on football results. Moreover, this part of the study also verifies hypothesis 4.\n  \n\n\nConclusion\n\nThe main finding of this article seems to be that confidence in intuition is the most important factor influencing intuitive versus nonintuitive decisions. Countermeasures are proposed such as artificially altering one’s confidence against their own intuition. Another possible explanation is that people tend to answer a relaxed version of a question (in this case, who will win instead of who will beat the point spread), or sometimes even a different question (which team do you prefer).\n",
        "content": "This blog post summarizes Simmons and Nelson’ study about how people incorporate constraining/contradicting information into their initial intuitive thought when asked to make a decision. It was written in the context of the Intro to Cognitive Science class. Intuitive Confidence: Choosing Between Intuitive and Nonintuitive Alternatives Authors: Joseph P. Simmons (Princeton University), Leif D. Nelson (New York University) Introduction People seem to favor intuitive options rather than equally or more valid nonintuitive options when taking decisions. This paper intends to explain how people weigh intuitive answers and nonintuitive alternatives that might oppose their initial intuition. It has been shown that people often prefer to follow their intuition even when conflicting with other available information, leading to judgment biases. Simmons and Nelson review the relevant related phenomena such as transparency illusions, beliefs in explicitly false statements and other biases appearing at different levels of human cognition. It seems plausible that two distinct systems are competing, the first one being responsible for fast, effortless, heuristics and knowledge based decisions whereas the second system - much slower and resource-demanding - attempts to correct the initial judgment using available cues, rules and reasoning. Some theories indicate that cognitive lazyness could prevent System 2 from kicking in and contributing to the decision. Others state that the sequential nature of the judgment process advantages the primity of the intuitive thought as System 2 has to falsify System 1’s conclusions persuasively. This phenomena is commonly refered to as anchoring and adjustment. However, such models can’t extensively explain the observed proportion of intuitive biases among motivated reasoners who fully process contradictory information. Simmons and Nelson propose a so-call dual-process model that supposedly explains the observed phenomena and makes relevant predictions. Simmon and Nelson’s model The authors’s model relies on four hypotheses: 1) Intuitions are chosen more often because people hold them with high Confidence 2) The magnitude of an opposing piece of information matters for invalidating intuition. 3) People who are more confident about their intuition will follow them more often 4) People who betray their intuition are less confident with their final choice. These rather natural hypotheses are then evaluated by the authors using predition of sporting events as a field case as it provides the experiment with the required variability of inputs magnitude and intuitive confidence. The example of the football bookmaker point spread concept is demonstrated, attempting to explain how most people handle this question. In this case, the initial intuition will answer the question which team will win and the point spread serves as the constraining information where further reasoning should be involved. All four hypotheses can be nicely instantiated against this scenario and evaluated against historical data. The following bullets summarize the result of Simmon and Nelson’s studies regarding the point spread example:  Most people bet on favorites in 90% of the games despite of the the point spread. Hypothesis 1 is verified (holds for rookies and experts)   The spread magnitude negatively impacts this proportion. Hyp 2 is correct (holds for rookies and experts)   People betting on underdogs showed less confidence on the outcome, which seems to verify hypothesis 4 is verified.   Finally, people bet for favorites even when setting the spread themselves. This study was design to eliminate the hypothesis that people simply don’t understand the point spread concept when betting on football results. Moreover, this part of the study also verifies hypothesis 4. Conclusion The main finding of this article seems to be that confidence in intuition is the most important factor influencing intuitive versus nonintuitive decisions. Countermeasures are proposed such as artificially altering one’s confidence against their own intuition. Another possible explanation is that people tend to answer a relaxed version of a question (in this case, who will win instead of who will beat the point spread), or sometimes even a different question (which team do you prefer). ",
        "url": "/2016/06/01/intuitive-vs-nonuntuitive-decisions/"
      },
    
      {
        "title": "Description of an Alembic",
        "excerpt": "The complete distilling apparatus consists of three parts: the “cucurbit” (Arabic ḳarʿa, Greek βίκος), the still pot containing the liquid to be distilled, which is heated by a flame; the “head” or “cap” (Arabic anbiḳ, Greek ἄμβιξ) which fits over the mouth of the cucurbit to receive the vapors, with an attached downward-sloping “tube” (Greek σωλήν), leading to the “receiver” (Arabic ḳābila, Greek ἄγγος or φιάλη) container.\n\n",
        "content": "The complete distilling apparatus consists of three parts: the “cucurbit” (Arabic ḳarʿa, Greek βίκος), the still pot containing the liquid to be distilled, which is heated by a flame; the “head” or “cap” (Arabic anbiḳ, Greek ἄμβιξ) which fits over the mouth of the cucurbit to receive the vapors, with an attached downward-sloping “tube” (Greek σωλήν), leading to the “receiver” (Arabic ḳābila, Greek ἄγγος or φιάλη) container. Retorts have the “cap” and the “cucurbit” made into one. The anbik is also called the raʾs (head) of the cucurbit. The liquid in the cucurbit is heated or boiled; the vapour rises into the anbik, where it cools by contact with the walls and condenses, running down the spout into the receiver. A modern descendant of the alembic is the pot still, used to produce distilled beverages. Originally from Alembic - Wikipedia ",
        "url": "/general/2016/08/27/example-post-one/"
      },
    
      {
        "title": "History of the Alembic",
        "excerpt": "Dioscorides’ ambix (described in his De materia medica) is a helmet-shaped lid for gathering condensed mercury. For Athenaeus (~ 225 C.E.) it is a bottle or flask. For later chemists it denotes various parts of crude distillation devices.\n\n",
        "content": "Dioscorides’ ambix (described in his De materia medica) is a helmet-shaped lid for gathering condensed mercury. For Athenaeus (~ 225 C.E.) it is a bottle or flask. For later chemists it denotes various parts of crude distillation devices. Alembic drawings appear in works of Cleopatra the Alchemist, Synesius, and Zosimos of Panopolis. There were alembics with two (dibikos) and three (tribikos) receivers.[4] According to Zosimos of Panopolis, the alembic was invented by Mary the Jewess.[5] The anbik is described by Ibn al-Awwam in his Kitab al-Filaha (Book of Agriculture), where he explains how rose-water is distilled. Amongst others, it is mentioned in the Mafatih al-Ulum (Key of Sciences) of Khwarizmi and the Kitab al-Asrar (Book of Secrets) of Al-Razi. Some illustrations occur in the Latin translations of works which are attributed to Geber.[2] Originally from Alembic - Wikipedia ",
        "url": "/history/2016/08/28/example-post-two/"
      },
    
      {
        "title": "Description of a Pot Still",
        "excerpt": "A pot still is a type of still used in distilling spirits such as whisky or brandy. Heat is applied directly to the pot containing the wash (for whisky) or wine (for brandy).\n",
        "content": "A pot still is a type of still used in distilling spirits such as whisky or brandy. Heat is applied directly to the pot containing the wash (for whisky) or wine (for brandy). This is called a batch distillation (as opposed to a continuous distillation). At standard atmospheric pressure, alcohol boils at 78 °C (172 °F), while water boils at 100 °C (212 °F). During distillation, the vapour contains more alcohol than the liquid. When the vapours are condensed, the resulting liquid contains a higher concentration of alcohol. In the pot still, the alcohol and water vapour combine with esters and flow from the still through the condensing coil. There they condense into the first distillation liquid, the so-called “low wines”. The low wines have a strength of about 25–35% alcohol by volume, and flow into a second still. It is then distilled a second time to produce the colourless spirit, collected at about 70% alcohol by volume. Colour is added through maturation in an oak aging barrel, and develops over time. The modern pot still is a descendant of the alembic, an earlier distillation device. ",
        "url": "/general/2016/08/29/example-post-three/"
      },
    
      {
        "title": "Welcome to Jekyll!",
        "excerpt": "You’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated.\n\nTo add new posts, simply add a file in the _posts directory that follows the convention YYYY-MM-DD-name-of-post.ext and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.\n\nJekyll also offers powerful support for code snippets:\n\ndef print_hi(name)\n  puts \"Hi, #{name}\"\nend\nprint_hi('Tom')\n#=&gt; prints 'Hi, Tom' to STDOUT.\n\nCheck out the Jekyll docs for more info on how to get the most out of Jekyll. File all bugs/feature requests at Jekyll’s GitHub repo. If you have questions, you can ask them on Jekyll Talk.\n\n",
        "content": "You’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated. To add new posts, simply add a file in the _posts directory that follows the convention YYYY-MM-DD-name-of-post.ext and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works. Jekyll also offers powerful support for code snippets: def print_hi(name) puts \"Hi, #{name}\" end print_hi('Tom') #=&gt; prints 'Hi, Tom' to STDOUT. Check out the Jekyll docs for more info on how to get the most out of Jekyll. File all bugs/feature requests at Jekyll’s GitHub repo. If you have questions, you can ask them on Jekyll Talk. ",
        "url": "/jekyll/update/2017/08/25/welcome-to-jekyll/"
      },
    
  
  
  
  {
    "title": "Categories",
    "excerpt": "Category index\n",
    "content": " ",
    "url": "/categories/"
  },
  
  {
    "title": "Sébastien Mamessier",
    "excerpt": "\n",
    "content": "CV/Resume Here you can find my CV in a pdf format. I’m currently working at Uber as an self driving technology engineer and wrapping up my pjs thesis. Before that I worked at Airbus on cockpit concepts and Virtual Reality as well as as a Graduate Researcher and Robotics PhD candidate at Georgia Tech School of Aerospace, working on adaptive controls for semi-autonomous cars as well as collaborative artificial intelligence for future commercial aviation cockpits. During my curriculum, I worked for two and a half years at Airbus Group Innovations in Munich, Germany, developing cockpit / flight automation concepts and evaluating them using Virtual Reality experiments. Before that, I received a M.S in Aerospace Engineering from Georgia Tech and a Diplome d’Ingenieur from Supaero (France) in 2013. My PhD thesis work focuses on computational modeling of safe and efficient Human-AI collaboration. Potential applications of my research span from flight deck automation to integration of adaptive controls in semi-automated cars. My fields of expertise and interests include Human Factors, Controls, Artificial Intelligence and Software Engineering. Private pilot since 2013, I’m a big soccer fan, practice astronomy and contribute to open source projects. ",
    "url": "/cv/"
  },
  
  {
    "title": "Elements",
    "excerpt": "A demo of Markdown and HTML includes\n",
    "content": "Heading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 A small element A link Lorem ipsum dolor sit amet, consectetur adip* isicing elit, sed do eiusmod *tempor incididunt ut labore et dolore magna aliqua. Duis aute irure dolor in A link reprehenderit in voluptate velit esse cillum bold text dolore eu fugiat nulla pariatur. Excepteur span element sint occaecat cupidatat non proident, sunt italicised text in culpa qui officia deserunt mollit anim id some code est laborum.  An item An item An item An item An item Item one Item two Item three Item four Item five A simple blockquote &lt;div&gt; &lt;span&gt;Some example code&lt;/span&gt; &lt;/div&gt; Single line of code HTML Includes Contact form   Name: *    Email Address: *    Message: *      \t\t\t\t* indicates a required field \t\t\t\t \t\t\t\t \t\t\t\t  Please enable JavaScript to use the form. Demo map embed  Button include A button A button with icon  Icon include Video include  Image includes  Image with caption  Right aligned image  Center aligned image  ",
    "url": "/elements/"
  },
  
  {
    "title": "Standard Deviation",
    "excerpt": "Standard Deviation is where I gather my thoughts and pieces of work that could benefit a broader audience one way or another. You will find academic projects, thoughts about new technologies, software tutorials as well as my resume.\n",
    "content": "Standard Deviation is where I gather my thoughts and pieces of work that could benefit a broader audience one way or another. You will find academic projects, thoughts about new technologies, software tutorials as well as my resume. ",
    "url": "/"
  },
  
  {
    "title": "Projects",
    "excerpt": "Sebastien Mamessier academic projects\n",
    "content": " ",
    "url": "/projects/"
  },
  
  {
    "title": "Search",
    "excerpt": "Search for a page or post you’re looking for\n",
    "content": "{% include site-search.html %} ",
    "url": "/search/"
  }
  
]

